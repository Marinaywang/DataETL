## Name Nodes

Eval old cluster: 
Name

Node
Original node: 
ssh yuexin_wang@mr11p01ms-hygq05183001.mps.apple.com

Two new nodes: 
ssh yuexin_wang@mr11p01ms-hygq05202001.mps.apple.com
ssh yuexin_wang@mr11p01ms-hygq05183701.mps.apple.com

Eval new cluster:
ssh mr85d01ls-hygq01131101.geo.apple.com

remote copy or remote access:

hadoop distcp -Dmapreduce.map.memory.mb=5120 -pb -update hdfs://st21d01ls-geo07061901.geo.apple.com:8020/apple_pay/weekly_drop/2018-12-04/ /eval/apple_pay_2018-12-04

Gemini Prod: st21d01ls-geo07061901.geo.apple.com

hadoop fs -D ipc.client.fallback-to-simple-auth-alowed=true -du -h hdfs://usspk17-eval-prod/

hadoop fs -D ipc.client.fallback-to-simple-auth-allowed=true -ls hdfs://st21-gemini-prod/apple_pay/weekly_drop/2018-12-04/

hadoop fs  --config /etc/hadoop-alt/conf distcp -D ipc.client.fallback-to-simple-auth-allowed=true -D mapreduce.map.memory.mb=5120 -pb -update hdfs://st21-gemini-prod/apple_pay/weekly_drop/2018-12-04/ /data/data_eval/apple_pay/2018_12_04

## prevent mac from sleeping
pmset noidle

## run query on the backend
 screen -DDR
 
## HIVE 
 show tables like ‘hm_*’;

SPARK initialization 

## Spark 


import org.apache.spark.sql.hive.orc._
import org.apache.spark.sql._
val hiveContext = new org.apache.spark.sql.hive.HiveContext(sc)

val test_enc_orc = hiveContext.sql("select * from default.gemini_c1710_g2017090521_staging2 limit 1")
test_enc_orc.printSchema()

test_enc_orc.show()

## KICKOFF JOB Scripts after set up 

sh setup.sh

cd /home/yuexin_wang/neutron-ndm-hive/clone_repo/marlin/
source /home/yuexin_wang/neutron-ndm-hive/clone_repo/marlin/venv/bin/activate

python /home/yuexin_wang/neutron-ndm-hive/clone_repo/marlin/src/python/repoTestingKH/kickoff_JB.py --job_type existing --job_id  3917fce1-0894-47ac-bc5d-378bf6e1f5d8 --env prod --user yuexin_wang --password Wyz1gsjkx+ --email marina.wang@apple.com

## Create a Hive table from seq file for basemap data

python create_table.py mwang.Repos18002_p2 /eval/data/spectrum_deepdive/copied_data/2a553aea-9a8e-4ecc-927f-7f11e58d0403/featurecontainer seq fcp

## Create a Hive table from json file (gemini or apple pay like)

create external table mwang.apple_pay_2018_12_04 (record string)
row format delimited fields terminated by '\001' stored as textfile 
location '/data/data_eval/apple_pay/2018_12_04';

## Explore this dataset

select b.placeid, s.name, s.transactionCount, s.merchantCategory, s.score from mwang.apple_pay_2018_12_04 a
LATERAL VIEW json_tuple(a.record, 'exportId', 'placeid', 'addedNames', 'removedNames', 'removeApplePay')
                              b as exportId, placeid, addedNames, removedNames, removeApplePay
   LATERAL VIEW explode(split(regexp_replace(regexp_replace(b.addedNames,'\\}\\,\\{','\\}mwang\\{'),'\\[|\\]',''), 'mwang')) d as details 
   LATERAL VIEW json_tuple(d.details, 'name', 'merchantCategory','transactionCount','score') s as name,merchantCategory, transactionCount, score
Limit 20; 
 
